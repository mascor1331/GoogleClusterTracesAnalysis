{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#/FileStore/tables/o42yj8mx1493971989213/job_events.csv\njobEventsDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/o42yj8mx1493971989213/job_events.csv\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["jobEventsDF = jobEventsDF.drop('_c1', '_c4', '_c6', '_c7').filter(\"_c0 > 600 * 1000000\").filter(\"_c0 < 9223372036854775807\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(jobEventsDF)\n\n#_c0 timestamp\n#_c2 jobID\n#_c3 eventType\n#_c5 schedulingClass"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["jobEventsDF.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["majorJobEventsDF = jobEventsDF.filter(\"_c3 == 1 or _c3 == 3 or _c3 == 4 or _c3 == 5\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(majorJobEventsDF)\n\n# 1 Schedule\n# 3 Fail\n# 4 Finish\n# 5 Kill"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["majorJobEventsClassDF = majorJobEventsDF.groupBy(['_c5', '_c3']).count().withColumnRenamed('_c3', 'Event Type').withColumnRenamed('_c5', 'Scheduling Class')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(majorJobEventsClassDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["majorJobEventsClassPDDF = majorJobEventsClassDF.toPandas()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["majorJobEventsClassPDDF = majorJobEventsClassPDDF.pivot(index='Scheduling Class', columns='Event Type', values='count')"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["majorJobEventsClassFig = majorJobEventsClassPDDF.plot.bar()\nmajorJobEventsClassFig.set_ylabel('Number of Job Events')\ndisplay(majorJobEventsClassFig.figure)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.functions import ceil\nmajorJobEventsDayDF = majorJobEventsDF.withColumn('day', ceil((majorJobEventsDF._c0 / 1000000 - 600) / 86400)).drop('_c0', '_c2', '_c5').groupBy(['day', '_c3']).count().withColumnRenamed('_c3', 'Event Type')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(majorJobEventsDayDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["majorJobEventsDayPDDF = majorJobEventsDayDF.toPandas()\nmajorJobEventsDayPDDF = majorJobEventsDayPDDF.pivot(index='day', columns='Event Type', values='count')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["majorJobEventsDayFig = majorJobEventsDayPDDF.plot.area(stacked=False)\nmajorJobEventsDayFig.set_xlabel('Day')\nmajorJobEventsDayFig.set_ylabel('Number of Job Events')\ndisplay(majorJobEventsDayFig.figure)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["jobTimeDF = jobEventsDF.withColumn('timestamp', (jobEventsDF._c0 / 1000000 - 600) / 60).drop('_c0')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(jobTimeDF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.functions import min\nfrom pyspark.sql.functions import max\n# 0 Submit\n# 1 Schedule\n# 4 Finish\nclass0TimeDF = jobTimeDF.filter(\"_c3 == 0\").groupBy(['_c5', '_c3', '_c2']).agg(min(jobTimeDF.timestamp)).drop('_c3').withColumnRenamed('min(timestamp)', 'submittime')\nclass1TimeDF = jobTimeDF.filter(\"_c3 == 1\").groupBy(['_c5', '_c3', '_c2']).agg(max(jobTimeDF.timestamp)).drop('_c5', '_c3').withColumnRenamed('max(timestamp)', 'scheduletime')\nclass4TimeDF = jobTimeDF.filter(\"_c3 == 4\").groupBy(['_c5', '_c3', '_c2']).agg(max(jobTimeDF.timestamp)).drop('_c5', '_c3').withColumnRenamed('max(timestamp)', 'finishtime')\n\n# jobTimeDF = jobTimeDF.groupBy(['_c5', '_c3', '_c2']).agg(functions.min(jobTimeDF.timestamp), functions.max(jobTimeDF.timestamp))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(class0TimeDF)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["jobDurationDF = class0TimeDF.join(class1TimeDF, '_c2', 'outer').join(class4TimeDF, '_c2', 'outer').na.fill(0)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(jobDurationDF)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.functions import ceil\n\njobExecDF = jobDurationDF.filter(\"submittime != 0 and scheduletime != 0 and finishtime != 0\").withColumn('totaltime', ceil(jobDurationDF.finishtime - jobDurationDF.submittime)).withColumn('runtime', ceil(jobDurationDF.finishtime - jobDurationDF.scheduletime)).drop('_c2', 'submittime', 'scheduletime', 'finishtime')\njobWaitDF = jobExecDF.withColumn('waittime', jobExecDF.totaltime - jobExecDF.runtime).drop('totaltime', 'runtime')"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(jobExecDF)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["display(jobWaitDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["jobExecPDDF = jobExecDF.groupBy(['_c5', 'runtime']).count().withColumnRenamed('_c5', 'Scheduling Class').toPandas()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["jobExecPDDF = jobExecPDDF.pivot(index='runtime', columns='Scheduling Class', values='count')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# jobExecPDDF = jobExecPDDF.cumsum()\njobExecFig = jobExecPDDF.plot(logx=True)\njobExecFig.set_xlabel('Runtime (min)')\njobExecFig.set_ylabel('Number of Jobs')\ndisplay(jobExecFig.figure)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["jobExecPDDF.describe()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["jobExecShortPDDF = jobExecDF.filter(\"runtime <= 300\").groupBy(['_c5', 'runtime']).count().withColumnRenamed('_c5', 'Scheduling Class').toPandas()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["jobExecShortPDDF = jobExecShortPDDF.pivot(index='runtime', columns='Scheduling Class', values='count')\njobExecShortFig = jobExecShortPDDF.plot(logy=True)\njobExecShortFig.set_xlabel('Runtime (min)')\njobExecShortFig.set_ylabel('Number of Jobs')\ndisplay(jobExecShortFig.figure)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["jobWaitPDDF = jobWaitDF.groupBy(['_c5', 'waittime']).count().withColumnRenamed('_c5', 'Scheduling Class').toPandas()\njobWaitPDDF = jobWaitPDDF.pivot(index='waittime', columns='Scheduling Class', values='count')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["jobWaitFig = jobWaitPDDF.plot(logx=True, logy=True)\njobWaitFig.set_xlabel('Wait Time (min)')\njobWaitFig.set_ylabel('Number of Jobs')\ndisplay(jobWaitFig.figure)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["jobWaitPDDF.describe()"],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"job-level","notebookId":3028920447636503},"nbformat":4,"nbformat_minor":0}
